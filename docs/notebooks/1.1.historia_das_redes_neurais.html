
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.6" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs-1.1/textmate.css"
      type="text/css" />

<script src="../site_libs/highlightjs-1.1/highlight.js"></script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Codando Deep Learning</title>

<style type = "text/css">
body {
  
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Codando Deep Learning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../notebooks.html">Notebooks</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="http://github.com/mari-linhares/codando-deep-learning"> Github </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.1-Introdu&#231;&#227;o-a-Redes-Neurais:-O-que-s&#227;o?">1.1 Introdu&#231;&#227;o a Redes Neurais: O que s&#227;o?<a class="anchor-link" href="#1.1-Introdu&#231;&#227;o-a-Redes-Neurais:-O-que-s&#227;o?">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hist&#243;ria-das-Redes-Neurais">Hist&#243;ria das Redes Neurais<a class="anchor-link" href="#Hist&#243;ria-das-Redes-Neurais">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="O-come&#231;o-de-tudo">O come&#231;o de tudo<a class="anchor-link" href="#O-come&#231;o-de-tudo">&#182;</a></h3><p>A história das redes neurais é curiosa, engraçada e triste ao mesmo tempo. Então, senta que lá vem história...</p>
<p>Certo dia, em 1958, um certo cientista nova-yorkino, chamado <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a>, inspirado pelo trabalho de seus colegas que estudavam sobre o cérebro humano, terminou de projetar um algoritmo que conseguia aprender sozinho a resolver problemas simples. Nascia ali, o <strong>Perceptron</strong> (que veremos mais adiante no curso).</p>
<p>A ideia inicial do Perceptron, na verdade, era ser uma máquina ao invés de um programa. E foi o que aconteceu. Utilizando o poderosíssimo <a href="https://en.wikipedia.org/wiki/IBM_704">IBM 704</a> que fazia 12 mil adições por segundo (só para você ter uma noção, um iPhone 7 faz 300 bilhões) os cientistas construíram uma máquina denominada <strong>Mark I Perceptron</strong>.</p>
<p><img align='center' src='https://github.com/mari-linhares/codando-deep-learning/blob/master/notebooks/images/mark_i.jpg?raw=true' width=600></p>
<p><strong>A ideia do Mark I Perceptron era que, para uma dada imagem de entrada, a máquina deveria aprender sozinha a classificar a saída em 0 ou 1</strong>. Mas, calma, nem imagens existiam naquela época! Então, como projetaram isso? Os cientistas na época conectaram 400 foto-células a entrada do Mark I para simular uma imagem 20x20 (um pouco maior talvez do que as letras desse texto). E para permitir a calibração da máquina, cada entrada dessa era conectada a um potenciômetro que eram ajustados automaticamente por motores elétricos para mapear as entradas na saída final 0 ou 1.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-winter-is-coming...-&#128532;">The winter is coming... &#128532;<a class="anchor-link" href="#The-winter-is-coming...-&#128532;">&#182;</a></h3><p>O Perceptron, na época, foi um sucesso! Afinal de contas, <strong>ele foi um dos primeiros algoritmos capazes de aprender sozinho</strong>. Além disso, já foi provado que <strong>o Perceptron tem garantia de sucesso quando as duas classes são linearmente separáveis</strong>. Porém, é aí que está o problema: o Perceptron nada mais é que um classificador binário linear. Ou seja, ele só funciona quando o seu problema é binário (2 classes) e seus dados podem ser separados por uma simples reta. Essas condições, no mundo real, são muito difíceis de acontecer. Só para exemplificar, como você separia os dados abaixo com apenas uma reta?</p>
<p><img align='center' src='https://jarvmiller.github.io/post/2017-10-20-neural-networks-units-and-decision-boundaries_files/figure-html/xor%20setup-1.png' width=400></p>
<p><a href="https://jarvmiller.github.io/2017/10/14/neural-nets-pt1/">Fonte da imagem</a></p>
<p>Pois é. O Perceptron também não consegue resolver esse simples problema (para quem não reparou, o gráfico acima representa a <a href="https://pt.wikipedia.org/wiki/Porta_XOR">porta XOR</a>). Na época, isso desanimou tanto os estudiosos da área de Inteligência Artificial, que pesquisas nessa área só foram retomadas de fato na década de 80. Tal período ficou conhecido como o <strong>inverno da Inteligência Artificial</strong> (<em>AI Winter</em>).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="O-salvador-da-p&#225;tria:-Geoffrey-Hinton">O salvador da p&#225;tria: Geoffrey Hinton<a class="anchor-link" href="#O-salvador-da-p&#225;tria:-Geoffrey-Hinton">&#182;</a></h3><p>O maior problema do Perceptron era que ele era apenas um só. Intuitivamente, é fácil perceber que um neurônio só não faz uma rede neural <del>(assim como uma andorinha só não faz verão)</del>. Pensando assim, muita gente tentou colocar um monte de Perceptrons conectados entre si para tentar resolver um problema. Porém, muitos do que fizeram isso se depararam com um problema: <strong>como propagar o erro da saída para as entradas?</strong> Em outras palavras, <strong>como fazer esse monte de Perceptrons aprenderem ao mesmo tempo sem que um atrapalhe o que o outro aprendeu?"</strong>.</p>
<p>Pensando nisso, o famoso <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a> desenvolveu o algoritmo <strong><a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a></strong> na década de 80. Utilizando o conceito de gradientes e regra da cadeia, tal algoritmo pega o erro de uma rede neural e propaga-o até as entradas, fazendo leves ajustes nos parâmetros (pesos) da rede. Com isso, Redes Neurais com mais de um neurônio e mais de uma camada poderiam começar a ser desenvolvidas e treinadas em problemas mais complexos. Como eu disse, <em>poderiam</em>...</p>
<p>O problema era que na década de 80, e mesmo na década de 90, o treinamento de tais redes e aplicação do backpropagation era muito pesado ainda. Mesmo supercomputadores se matavam para treinar e executar tais redes ainda. Então, o que fazer?</p>
<h3 id="Obrigado,-gamers!">Obrigado, gamers!<a class="anchor-link" href="#Obrigado,-gamers!">&#182;</a></h3><p>Mais uma vez o mundo foi salvo graças aos <strong>gamers</strong>. Isso mesmo. Essa obsessão dos gamers em sempre querer computadores mais potentes e jogos com gráficos cada vez melhores, fez com que a indústria dos computadores, especialmente a das GPUs se desenvolvessem num ritmo assustador - regido pela Lei de Murphy. Mas, o que os jogos têm a ver com o desenvolvimento das Redes Neurais?</p>
<p>A resposta é: matrizes! Como vamos ver num dos próximos assuntos, <strong>Redes Neurais tem tudo a ver com matrizes</strong>. Basicamente, Redes Neurais fazem um monte de cálculo sobre matrizes, como: soma, multiplicão, operaçõe ponto-a-ponto, etc... E, como GPUs são computadores especializados em cálculos sobre matrizes, o campo das Redes Neurais pode se desenvolver como nunca. Cada vez mais, redes mais complexas e pesadas puderam ser desenvolvidas e treinadas.</p>
<hr>
<p>Agora que já tivemos uma introdução sobre a história das Redes Neurais, chegou a hora de aprendermos mais sobre elas. Então, prepara um café que chegou a hora de começar os estudos...</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Refer&#234;ncias">Refer&#234;ncias<a class="anchor-link" href="#Refer&#234;ncias">&#182;</a></h2><p>Este conteúdo é baseado nos seguintes materiais:</p>
<ul>
<li><a href="https://github.com/iamtrask/Grokking-Deep-Learning">Capítulo 3</a> de Grokking Deep Learning.</li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron">Perceptron</a> da Wikipedia</li>
<li><a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> da Wikipedia</li>
<li><a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a> da Wikipedia</li>
<li><a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation</a> da Wikipedia</li>
</ul>

</div>
</div>
</div>
<hr>
&copy; 2019 Marianne Linhares, Arnaldo Gualberto

</div>
</div>
</body>
</html>
