{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introdução a Redes Neurais: O que são?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é uma rede neural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais podem ser vistas de maneira bastante simplificada como **funções matemáticas**. Uma simples definição de redes neurais é:\n",
    "\n",
    "> \"Redes Neurais são *aproximadores de função*\"\n",
    "\n",
    "Desse modo, similar à funções, as redes neurais possuem entradas e saídas. As entradas são representadas pelas amostras dos seus dados. Em geral, elas são matrizes $NxD$, onde $N$ (#linhas) **representa o número de amostras** que seu banco de dados tem e $D$ (#colunas) **representa a quantidade de atributos** de cada amostra, também conhecida por *dimensionalidade*. Como exemplo, imagine que tenhamos um banco de dados com 1.000 amostras e cada amostra tem 5 atributos. Logo, nossas entradas seriam representadas por uma matriz $1000x5$, sacou? \n",
    "\n",
    "**As saídas, por sua vez, representam o que você quer que a sua rede aprenda**. Por exemplo: dado um banco com a altura de determinadas pessoas (entrada), queremos estimar o \"peso\" dessas pessoas. Nesse caso, o \"peso\" das pessoas é a variável que queremos estimar. Portanto, o \"peso\" nesse caso representaria a nossa saída. Sempre que a nossa saída é conhecida, nós dizemos que esse tipo de problema é um problema de **Aprendizagem Supervisionada**. Há casos em que não necessariamente o nosso problema tem uma saída explícita. Nesse caso, teremos uma **Aprendizagem Não-Supervisionada**. Além disso, quando a **saída assume qualquer valor real** (0, 1.2, 3.14, -26, +34, ...), nós dizemos que temos um **Problema de Regressão**. Por outro lado, quando a **saída é discreta** (0/1, homem/mulher, cachorro/gato/passarinho), nós temos **Problemas de Classificação**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura de Redes Neurais\n",
    "\n",
    "Redes Neurais são definidas em termos de camadas. Em geral, **a primeira camada representa a entrada da rede, enquanto a última camada representa a saída**. Todas as camadas que estão entre as camadas de entrada e saída são chamadas de **camadas escondidas** (ou *hidden layers*). Um exemplo de uma Rede Neural com 2 camadas escondidas pode ser vista na figura abaixo:\n",
    "\n",
    "<img align='center' src='https://cdn-images-1.medium.com/max/1200/0*hzIQ5Fs-g8iBpVWq.jpg' width=500>\n",
    "\n",
    "[Fonte da imagem](https://cdn-images-1.medium.com/max/1200/0*hzIQ5Fs-g8iBpVWq.jpg)\n",
    "\n",
    "Com exceção da camada de entrada, toda camada de uma rede é composta pela seguintes propriedades:\n",
    "\n",
    "- **número de neurônios**: na figura acima, as duas camadas escondidas tem 4 neurônios, enquanto a camada de saída tem apenas 1.\n",
    "- **parâmetros**: cada neurônio recebe como entrada todos as saídas dos neurônios das camadas anteriores. Cada entrada dessa é multiplicada por um peso correspondente. **Tais pesos representam o que a Rede Neural pode ajustar para encontrar a solução do problema e são conhecidos como parâmetros**. \n",
    "\n",
    "> ⚠️ **Cuidado: não confunda parâmetro com hiperparâmetros!** Parâmetros são o que a sua rede usa para aprender (pesos e bias), enquanto hiperparâmetros são o que você define acerca da sua rede (número de camadas, qtde. de neurônios por camada, função de ativação de cada camada, etc...)\n",
    "\n",
    "- **função de ativação**: cada neurônio da rede tem uma função de ativação embutida. Elas são responsáveis por dar o poder de não-linearidade à rede - quando você usa uma função de ativação não-linear (*sigmoid*, *tanh*, *ReLU*, etc...), obviamente. Nós estudaremos sobre elas mais à frente um pouco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, weights=0.5):\n",
    "        self._weights = weights\n",
    "    \n",
    "    def function(self, _input):\n",
    "        return self._activation_function(_input * self._weights)\n",
    "    \n",
    "    def _activation_function(self, data):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyFirstNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Este conteúdo é baseado nos seguintes materiais:\n",
    "\n",
    "- [Capítulo 3](https://github.com/iamtrask/Grokking-Deep-Learning) de Grokking Deep Learning.\n",
    "- [Perceptron](https://en.wikipedia.org/wiki/Perceptron) da Wikipedia\n",
    "- [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) da Wikipedia\n",
    "- [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) da Wikipedia\n",
    "- [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) da Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
