{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introdução a Redes Neurais: o que são?\n",
    "\n",
    "\n",
    "Nesta seção iremos discutir o que são redes neurais e como implementá-las de forma eficiente utilizando matrizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais são aproximadores de funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais podem ser vistas de maneira bastante simplificada como **funções matemáticas**.\n",
    "\n",
    "**Ideia abstrata**\n",
    "\n",
    "```\n",
    "Entradas: x ----> Computação: f ------> Saídas: f(x)\n",
    "```\n",
    "\n",
    "**Matemática**\n",
    "\n",
    "$f(x) = x \\times 2 + 3$\n",
    " \n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "def f(x):\n",
    "    return x * 2 + 3\n",
    "```\n",
    "\n",
    "\n",
    "Similar à funções, as redes neurais possuem entradas e saídas. As entradas são representadas por amostras dos seus dados. As saídas, por sua vez, dependem de que tarefa estamos desempenhando.\n",
    "\n",
    "Por exemplo, se estamos treinando uma rede neural para classificar imagens que contém cachorros ou gatos, as entradas serão imagens (matrizes de píxels) e as saídas podem ser simplismente um valor binário (1 se tiver um gato na imagem e 0 se tiver um cachorro). A questão é: não sabemos a melhor função `f` para mapear as entradas para as saídas. Podemos treinar uma rede neural para achar essa função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, uma simples definição de redes neurais é:\n",
    "\n",
    "> \"Redes Neurais são *aproximadores de funções*\"\n",
    "\n",
    "A diferenção é que não sabemos qual a função ótima para representar uma entrada:\n",
    "\n",
    "```python\n",
    "def f(x):\n",
    "       return x * ? + ??\n",
    "```\n",
    "\n",
    "Não sabemos qual o melhor `f` possível, mais especificamente, quais valores deveriam ser os valores de `?` e `??`?\n",
    "\n",
    "Poderíamos tentar várias funções para se **ajustar** aos dados, quando treinamos rede neurais estamos na verdade buscando **aprender** quais são os melhores **parâmetros** (aqui representados por `?` e `??`) para a uma certa função função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais são construídas a partir de camadas\n",
    "\n",
    "\n",
    "Redes Neurais são definidas em termos de camadas. **A primeira camada representa as entradas da rede, enquanto a última camada representa as saídas**.\n",
    "\n",
    "Todas as camadas que estão entre as camadas de entrada e saída são chamadas de **camadas escondidas** (ou *hidden layers*). Um exemplo de uma Rede Neural com 2 camadas escondidas pode ser vista na figura abaixo:\n",
    "\n",
    "![](images/rede_neural.png)\n",
    "\n",
    "### Sobre as camadas...\n",
    "\n",
    "\n",
    "Com exceção da camada de entrada, toda camada de uma rede é composta pela seguintes propriedades:\n",
    "\n",
    "- **número de neurônios**: na figura acima, a primeira camada escondida tem 4 neurônios, já a segunda camada escondida tem 3 neurônios, enquanto a camada de saída tem apenas 1. Cada neurônio representa um valor.\n",
    "\n",
    "- **parâmetros**: cada neurônio recebe como entrada todos as saídas dos neurônios das camadas anteriores. Cada entrada dessa é multiplicada por um peso correspondente. **Tais pesos representam o que a Rede Neural pode ajustar para encontrar a solução do problema e são conhecidos como parâmetros**.\n",
    "\n",
    "\n",
    "**A imagem acima mostra apenas uma arquitetura possível para uma rede neural.** Existem vários outros tipos de arquiteturas e outros tipos de camadas que podemos usar para construir uma rede neural. Nessa parte do curso iremos focar nessa arquitetura que se chama [MultiLayerPerceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
    "\n",
    "Todas essas camadas são chamadas **camadas densas**. A arquitetura MLP é caracterizada por várias camadas densas uma após a outra (várias camadas intermediárias). Apesar de na imagem acima termos escolhido mostrar apenas 2 camadas intermediárias poderíamos ter quantas quiséssemos, com quantos nós quiséssemos! \n",
    "\n",
    "O número de nós de entrada e de saída são fixos dependendo do problema que estamos tentando resolver. Por exemplo, no problema da classificação de imagens de cachorros e gatos, o número de neurônios de entrada seria o número de píxeis da imagem e o número de neurônios na camada de saída um único neurônio (0 ou 1).\n",
    "\n",
    "\n",
    "### Entendendo a matemática\n",
    "\n",
    "Vamos focar apenas na computação que envolve o segundo nó da segunda camada intermediária:\n",
    "\n",
    "![](images/focando_rede_neural.png)\n",
    "\n",
    "\n",
    "Cada peso (linha conectando os neurônios da camada anterior a esse neurônio) é um número, e cada nó também representa um número.\n",
    "\n",
    "![](images/focando_rede_neural2.png)\n",
    "\n",
    "Interpretamos esse número como a saída do neurônio, se a saída é um valor alto (> 0) dizemos que o neurônio **foi ativado**. Como a imagem sugere, utilizamos a saída dos neurônios anteriores para calcular a saída do próximo neurônio. Mas como? \n",
    "\n",
    "\n",
    "$$SN_j = f(\\sum{(SN_i \\cdot pij)} + b_j)$$\n",
    "\n",
    "$SN_j$ = Saída do Neurônio j  \n",
    "$P_ij$ = Peso que liga o neurônio i ao neurônio j  \n",
    "$b_j$ = Bias de do neurônio j  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def funcao_de_ativacao(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def liga_neuronios(saida_neuronios_da_camada_anterior, pesos_neuronio, peso_bias, f):\n",
    "    saida_neuronio = 0\n",
    "    \n",
    "    # Soma ponderada dos pesos com a saída\n",
    "    for saida_neuronio_anterior, peso in zip(saida_neuronios_da_camada_anterior, pesos_neuronio):\n",
    "        saida_neuronio += saida_neuronio_anterior * peso\n",
    "    \n",
    "    # Adiciona bias\n",
    "    saida_neuronio += peso_bias\n",
    "    \n",
    "    # Aplica função de ativação\n",
    "    saida_neuronio = f(saida_neuronio)\n",
    "    \n",
    "    return saida_neuronio\n",
    "\n",
    "# Exemplo da imagem\n",
    "liga_neuronios([4.1, 2.9, 0.7, -0.3], [0.1, 3.7, -1.3, 4.1], 1, funcao_de_ativacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ok... então para calcular a saída de um neurônio, eu faço a soma ponderada entre os pesos e todas as saídas dos neurônios da camada anterior. Ate aí tudo bem... mas aí eu somo a isso uma coisa chamada bias e depois aplico sobre esse valor uma função de ativação?\n",
    "\n",
    "Isso!\n",
    "\n",
    "![](https://raw.githubusercontent.com/arnaldog12/Manual-Pratico-Deep-Learning/master/images/perceptron.png)\n",
    "Imagem de: https://github.com/arnaldog12/Manual-Pratico-Deep-Learning/blob/master/Perceptron.ipynb\n",
    "\n",
    "\n",
    "> OBS: Se isolarmos um nó da rede neural temos o que é chamado **perceptron**.\n",
    "\n",
    "\n",
    "### Função de Ativação\n",
    "\n",
    "Como o próprio nome indica é uma função matemática. Por enquanto não vamos nos preocupar muito com ela, iremos ver mais sobre funções de ativação na seção 1.5.\n",
    "\n",
    "Por enquanto só vou dizer 2 coisas: \n",
    "\n",
    "   * A função de ativação que utilizamos no código anterior se chama ReLU, é uma das mais utilizadas e é comumente utilizada entre as camadas intermediárias. Na camada final geralmente não utilizamos função de ativação ou utilizamos alguma outra função (não relu).\n",
    "   \n",
    "   \n",
    "![](https://cdn-images-1.medium.com/max/937/1*oePAhrm74RNnNEolprmTaQ.png) \n",
    "    \n",
    "    \n",
    "    \n",
    "   * Como experado de uma função, uma função de ativação transforma uma entrada em alguma outra coisa que obtemos como saída. No caso das funções de ativação é bastante interessante que ela não seja uma reta, por exemplo na imagem acima temos com a ReLU se comporta, ta vendo que ela é tipo um cotovelo dobrado no zero? Isso faz com que a função não seja uma linha reta, e é isso que queremos na função de ativação, não precisa ser um cotovelo, poderia ser uma barraguinha, mas precisamos de curvas!\n",
    "    ![](https://www.bepantol.com.br/static/media/images/sobrePele-Produtos/cotovelo-ressecado.jpg)\n",
    "\n",
    "\n",
    "### Bias\n",
    "\n",
    "O bias é um termo independente da entrada que existe para nos dar mobilidade!\n",
    "\n",
    "Por exemplo, considere a seguinte função:\n",
    "\n",
    "y = 2 * x\n",
    "\n",
    "Essa função tem uma mobilidade limitada, ela passa pela origem (se x = 0, y = 0) e não podemos mudar isso :(. Porém se essa função fosse:\n",
    "\n",
    "y = 2 * x + bias\n",
    "\n",
    "Ao modificar o bias, podemos fazer com que a função vá mais para esquerda ou direita, não mais sendo forçada a passar pela origem.\n",
    "\n",
    "Na nossa rede o bias faz um papel similar! Nos dando liberdade para mover a função que estamos aprendendo, lembra que redes neurais são aproximadores de funções?\n",
    "\n",
    "[Mais informações sobre bias (em inglês) nesse link](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks).\n",
    "\n",
    "\n",
    "**Importante**\n",
    "\n",
    "Nos diagramas que mostramos, o bias não era mostrado, por que? Geralmente pra deixar a visualização mais bonitinha não mostramos o bias, abaixo segue nossa rede reural com o bias no diagrama:\n",
    "\n",
    "![](images/bias_rede_neural.png) \n",
    "\n",
    "O bias é um nó na rede como qualquer outro, porém ele não se conecta aos nós da camada anterior, podemos pensar que seu valor é fixo e igual a 1.\n",
    "\n",
    ">*“Se hoje é o Dia da rede neural, ontem eu disse que rede neural... o dia da rede neural é dia da camada, do peso e das funções de ativação, mas também é o dia dos bias. Sempre que você olha uma camada densa, há sempre uma figura oculta, que é um bias atrás, o que é algo muito importante.”* - Marianne Rousseff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jutando os pedaços e codando uma Rede Neural\n",
    "\n",
    "Vamos codar a seguinte rede neural:\n",
    "\n",
    "![](images/codando_rede_neural.png)\n",
    "\n",
    "Nomeamos os neurônios pra ficar mais simples de identificá-los no código.\n",
    "\n",
    "   * ni_cj = i-ésimo neuronio da j-ésima camada\n",
    "\n",
    "\n",
    "> Os valores escolhidos são arbitrários e a saída não significa nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.00099999999999\n"
     ]
    }
   ],
   "source": [
    "# ------- Camada de entrada -------\n",
    "# Definimos esse valor! Na prática irá vir de uma base de dados.\n",
    "entrada = -3\n",
    "\n",
    "# ------- Primeira camada ---------\n",
    "n1_c1 = liga_neuronios([entrada], [-4.3], 1, funcao_de_ativacao)\n",
    "n2_c1 = liga_neuronios([entrada], [-1.3], 2.2, funcao_de_ativacao)\n",
    "\n",
    "# ------- Segunda camada ---------\n",
    "n1_c2 = liga_neuronios([n1_c1, n2_c1], [0.2, 3.3], 2.7, funcao_de_ativacao)\n",
    "\n",
    "# ------- Camada de saída --------\n",
    "\n",
    "# Na última camada não usamos ReLU, lembra? Podemos simplesmente não usar\n",
    "# função de ativação, para tal é só usar uma função que não faz nada (função identidade)\n",
    "# essa função é também denominada como linear nos frameworks de Deep Learning.\n",
    "def faz_nada(x):\n",
    "    return x\n",
    "\n",
    "saida = liga_neuronios([n1_c2], [4.1], -10, faz_nada)\n",
    "\n",
    "print(saida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grande sacada das redes neurais é a definição de camadas e como elas se comunicam. Uma rede neural consiste basicamente da mesma computação baseada em camadas onde a entrada de uma camada é a camada imediamente anterior a ela.\n",
    "\n",
    "\n",
    "Ou seja: uma camada aprende a partir das camadas anteriores! \n",
    "\n",
    "\n",
    "Isso permite que a rede neural busque modificar os pesos de uma camada de maneira a gerar representações melhores para que a próxima camada tenha acesso a informações mais relevantes pra resolver o problema!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deixando o código eficiente, bonito e cheiroso\n",
    "\n",
    "Como a(o) boa(bom) programadora(programador) que você é, deve ter notado que o código anterior é bastante ineficiente!!! \n",
    "\n",
    "**PERGUNTA: mais espeficamente para cada nó em uma rede neural quantas operações são necessárias para calcular sua saída?**\n",
    "\n",
    "Pense a respeito, a resposta segue abaixo...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular a saída desse nó precisamos cacular a soma ponderada de todos os nós da camada anterior, digamos que a camada anterior tem N nós.\n",
    "\n",
    "Então precisaríamos de O(N) operações para calcular a saída de um único nó nessa camada.\n",
    "\n",
    "Então, por exemplo, se tivermos 1 camada de entrada com 100 nós e 2 camadas intermediárias com 200 nós cada, precisaríamos realizar o seguinte número de operações em cada camada:\n",
    "\n",
    "* camada intermediária 1: 100 * 200 (100 entradas para cada nó, 200 nós) = 20000\n",
    "* camada intermediária 2: 200 * 200 (200 entradas para cada nó, 200 nós) = 40000\n",
    "\n",
    "Totalizando: 60000 operações para calcular todas as saídas.\n",
    "\n",
    "Isso é péssimo :((! Queremos várias camadas na nossa rede, não podemos ser tão lentos :((\n",
    "\n",
    "Felizmente temos uma solução para deixar esse cálculo mais eficiente!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/freeze/max/1000/1*W8c9Eg-rJgLVhPmx4TD3Rw.jpeg?q=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que:\n",
    "\n",
    "1. Cada nó em uma camada é independente dos demais! Então podemos \"paralelizar\" essa computação!\n",
    "2. Uma camada L precisa de todos os nós da camada L-1 computados.\n",
    "\n",
    "![](images/focando_rede_neural2.png)\n",
    "\n",
    "\n",
    "Agora vem a mágica: 3. Podemos computar todos os valores de uma camada através de multiplicação de matrizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.kastatic.org/googleusercontent/rk4fR1jNJsGUfdHOc87UzuQh2zokwYDoVo3Hk1m3s6ToGDgW6KxgrsUeIj8-CJeV6cNf6WB8B6sRHt3BoGBdVY7h)\n",
    "\n",
    "[Imagem de Khan Academy](https://www.google.com/url?sa=i&source=images&cd=&ved=2ahUKEwjn4ITk1eHiAhXiHbkGHc0XCYsQjRx6BAgBEAU&url=https%3A%2F%2Fpt.khanacademy.org%2Fmath%2Fprecalculus%2Fprecalc-matrices%2Fmultiplying-matrices-by-matrices%2Fa%2Fmultiplying-matrices&psig=AOvVaw3HY59G-fpqGsmbzJ-GztER&ust=1560350722426497)\n",
    "\n",
    "![](images/mult_matriz.png)\n",
    "\n",
    "### Por que utilizar multiplicação de matrizes é mais eficiente?\n",
    "\n",
    "1. GPUs manipulam matrizes de forma extremamente eficiente!\n",
    "2. Existem otimizações para lidar com esse tipo de operação [de maneira eficiente](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms).\n",
    "3. Multiplicação de matrizes é altamente paralelizável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codando uma rede neural com multiplicação de matrizes\n",
    "\n",
    "Vamos modificar o código que fizemos anteriormente para utilizar multiplicação de matrizes.\n",
    "\n",
    "\n",
    "![](images/codando_rede_neural.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipula vetores e matrizes\n",
    "import numpy as np\n",
    "\n",
    "class DenseLayer(object):\n",
    "    def __init__(self, activation_function):\n",
    "        self._weights = []\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        self._weights = weights\n",
    "        \n",
    "    def _add_bias(self, _input):\n",
    "        # Adiciona 1's a entrada\n",
    "        return np.column_stack((np.ones(len(_input)), _input))\n",
    "    \n",
    "    def __call__(self, _input):\n",
    "        # adiciona bias a entrada, lembra?\n",
    "        input_with_bias = self._add_bias(_input)\n",
    "        # f_act(entrada * transposta(pesos))\n",
    "        return self.activation_function(np.matmul(input_with_bias, self._weights.transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes\n",
    "# def funcao_de_ativacao(x):\n",
    "#     if x > 0:\n",
    "#         return x\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# Agora\n",
    "def funcao_de_ativacao(x):\n",
    "    return np.where(x > 0, x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 95.001]]\n"
     ]
    }
   ],
   "source": [
    "# ------- Camada de entrada -------\n",
    "# Definimos esse valor! Na prática irá vir de uma base de dados.\n",
    "entrada = -3\n",
    "\n",
    "# ------- Primeira camada ---------\n",
    "# Antes\n",
    "# n1_c1 = liga_neuronios([entrada], [-4.3], 1, funcao_de_ativacao)\n",
    "# n2_c1 = liga_neuronios([entrada], [-1.3], 2.2, funcao_de_ativacao)\n",
    "\n",
    "# Agora\n",
    "camada1 = DenseLayer(funcao_de_ativacao)\n",
    "camada1.set_weights(np.array([[1, -4.3], [2.2, -1.3]]))\n",
    "\n",
    "# ------- Segunda camada ---------\n",
    "# Antes\n",
    "# n1_c2 = liga_neuronios([n1_c1, n2_c1], [0.2, 3.3], 2.7, funcao_de_ativacao)\n",
    "\n",
    "# Agora\n",
    "camada2 = DenseLayer(funcao_de_ativacao)\n",
    "camada2.set_weights(np.array([[2.7, 0.2, 3.3]]))\n",
    "\n",
    "\n",
    "# ------- Camada de saída --------\n",
    "\n",
    "# Na última camada não usamos ReLU, lembra? Podemos simplesmente não usar\n",
    "# função de ativação, para tal é só usar uma função que não faz nada (função identidade)\n",
    "# essa função é também denominada como linear nos frameworks de Deep Learning.\n",
    "def faz_nada(x):\n",
    "    return x\n",
    "\n",
    "# Antes\n",
    "# saida = liga_neuronios([n1_c2], [4.1], -10, faz_nada)\n",
    "\n",
    "# Agora\n",
    "ultima_camada = DenseLayer(faz_nada)\n",
    "ultima_camada.set_weights(np.array([[-10, 4.1]]))\n",
    "\n",
    "saida = ultima_camada(camada2(camada1(np.array([entrada]))))\n",
    "\n",
    "print(saida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "- [Capítulo 3](https://github.com/iamtrask/Grokking-Deep-Learning) de Grokking Deep Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
