{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introdução a Redes Neurais: o que são?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais são aproximadores de funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais podem ser vistas de maneira bastante simplificada **funções matemáticas**.\n",
    "\n",
    "**Ideia abstrata**\n",
    "\n",
    "```\n",
    "Entradas: x ----> Computação: f ------> Saídas: f(x)\n",
    "```\n",
    "\n",
    "**Matemática**\n",
    "\n",
    "$f(x) = x \\times 2 + 3$\n",
    " \n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "def f(x):\n",
    "    return x * 2 + 3\n",
    "```\n",
    "\n",
    "\n",
    "Similar à funções, as redes neurais possuem entradas e saídas. As entradas são representadas por amostras dos seus dados. As saídas, por sua vez, dependem de que tarefa estamos desempenhando.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, uma simples definição de redes neurais é:\n",
    "\n",
    "> \"Redes Neurais são *aproximadores de funções*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais são construídas a partir de camadas\n",
    "\n",
    "\n",
    "Redes Neurais são definidas em termos de camadas. **A primeira camada representa as entradas da rede, enquanto a última camada representa as saídas**.\n",
    "\n",
    "Todas as camadas que estão entre as camadas de entrada e saída são chamadas de **camadas escondidas** (ou *hidden layers*). Um exemplo de uma Rede Neural com 2 camadas escondidas pode ser vista na figura abaixo:\n",
    "\n",
    "![](images/rede_neural.png)\n",
    "\n",
    "### Sobre as camadas...\n",
    "\n",
    "\n",
    "Com exceção da camada de entrada, toda camada de uma rede é composta pela seguintes propriedades:\n",
    "\n",
    "- **número de neurônios**: na figura acima, a primeira camada escondida tem 4 neurônios, já a segunda camada escondida tem 3 neurônios, enquanto a camada de saída tem apenas 1. Cada neurônio representa um valor.\n",
    "\n",
    "- **parâmetros**: cada neurônio recebe como entrada todos as saídas dos neurônios das camadas anteriores. Cada entrada dessa é multiplicada por um peso correspondente. **Tais pesos representam o que a Rede Neural pode ajustar para encontrar a solução do problema e são conhecidos como parâmetros**.\n",
    "\n",
    "\n",
    "**A imagem acima mostra apenas uma arquitetura possível para uma rede neural.** Existem vários outros tipos de arquiteturas e outros tipos de camadas que podemos usar para construir uma rede neural. Nessa parte do curso iremos focar nessa arquitetura que se chama [MultiLayerPerceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
    "\n",
    "Todas essas camadas são chamadas **camadas densas**. A arquitetura MLP é caracterizada por várias camadas densas uma após a outra (várias camadas intermediárias). Apesar de na imagem acima termos escolhido mostrar apenas 2 camadas intermediárias poderíamos ter quantas quiséssemos, com quantos nós quiséssemos! O número de nós de entrada e de saída são fixos dependendo do problema que estamos tentando resolver.\n",
    "\n",
    "\n",
    "### Entendendo a matemática\n",
    "\n",
    "Vamos focar apenas na computação que envolve o segundo nó da segunda camada intermediária:\n",
    "\n",
    "![](images/focando_rede_neural.png)\n",
    "\n",
    "\n",
    "Cada peso (linha conectando os neurônios da camada anterior a esse neurônio) é um número, e cada nó também representa um número.\n",
    "\n",
    "![](images/focando_rede_neural2.png)\n",
    "\n",
    "Interpretamos esse número como a saída do neurônio, se a saída é um valor alto (> 0) dizemos que o neurônio **foi ativado**. Como a imagem sugere, utilizamos a saída dos neurônios anteriores para calcular a saída do próximo neurônio. Mas como? \n",
    "\n",
    "\n",
    "$$SN_j = f(\\sum{(SN_i \\cdot pij)} + b_j)$$\n",
    "\n",
    "$SN_j$ = Saída do Neurônio j  \n",
    "$P_ij$ = Peso que liga o neurônio i ao neurônio j  \n",
    "$b_j$ = Bias de do neurônio j  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def funcao_de_ativacao(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def liga_neuronios(saida_neuronios_da_camada_anterior, pesos_neuronio, bias, f):\n",
    "    saida_neuronio = 0\n",
    "    \n",
    "    # Soma ponderada dos pesos com a saída\n",
    "    for saida_neuronio_anterior, peso in zip(saida_neuronios_da_camada_anterior, pesos_neuronio):\n",
    "        saida_neuronio += saida_neuronio_anterior * peso\n",
    "    \n",
    "    # Adiciona bias\n",
    "    saida_neuronio += bias\n",
    "    \n",
    "    # Aplica função de ativação\n",
    "    saida_neuronio = f(saida_neuronio)\n",
    "    \n",
    "    return saida_neuronio\n",
    "\n",
    "# Exemplo da imagem\n",
    "liga_neuronios([4.1, 2.9, 0.7, -0.3], [0.1, 3.7, -1.3, 4.1], 1, funcao_de_ativacao)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ok... então para calcular a saída de um neurônio, eu faço a soma ponderada entre os pesos e todas as saídas dos neurônios da camada anterior. Ate aí tudo bem... mas aí eu somo a isso uma coisa chamada bias e depois aplico sobre esse valor uma função de ativação?\n",
    "\n",
    "Isso!\n",
    "\n",
    "![](https://raw.githubusercontent.com/arnaldog12/Manual-Pratico-Deep-Learning/master/images/perceptron.png)\n",
    "Imagem de: https://github.com/arnaldog12/Manual-Pratico-Deep-Learning/blob/master/Perceptron.ipynb\n",
    "\n",
    "\n",
    "> OBS: Se isolarmos um nó da rede neural temos o que é chamado **perceptron**.\n",
    "\n",
    "\n",
    "### Função de Ativação\n",
    "\n",
    "Como o próprio nome indica é uma função matemática. Por enquanto não vamos nos preocupar muito com ela, iremos ver mais sobre funções de ativação na seção 1.5.\n",
    "\n",
    "Por enquanto só vou dizer 2 coisas: \n",
    "\n",
    "   * A função de ativação que utilizamos no código anterior se chama ReLU, é uma das mais utilizadas e é comumente utilizada entre as camadas intermediárias. Na camada final geralmente não utilizamos função de ativação ou utilizamos alguma outra função (não relu).\n",
    "    \n",
    "    \n",
    "   * Como experado de uma função, uma função de ativação transforma uma entrada em alguma outra coisa que obtemos como saída. No caso das funções de ativação é bastante interessante que ela não seja uma reta, por exemplo na imagem abaixo temos com a ReLU se comporta, ta vendo que ela é tipo um cotovelo dobrado no zero? Esse cotovelo faz com que a reta que define essa função não seja uma linha reta, e é isso que queremos na função de ativação, não precisa ser um cotovelo, poderia ser uma barraguinha, mas precisamos de curvas!\n",
    "    \n",
    "![](https://cdn-images-1.medium.com/max/937/1*oePAhrm74RNnNEolprmTaQ.png) ![](https://www.bepantol.com.br/static/media/images/sobrePele-Produtos/cotovelo-ressecado.jpg)\n",
    "\n",
    "\n",
    "### Bias\n",
    "\n",
    "O bias é um termo independente da entrada que existe para nos dar mobilidade!\n",
    "\n",
    "Por exemplo, considere a seguinte função:\n",
    "\n",
    "y = 2 * x\n",
    "\n",
    "Essa função tem uma mobilidade limitada, ela passa pela origem (se x = 0, y = 0) e não podemos mudar isso :(. Porém se essa função fosse:\n",
    "\n",
    "y = 2 * x + bias\n",
    "\n",
    "Ao modificar o bias, podemos fazer com que a função vá mais para esquerda ou direita, não mais sendo forçada a passar pela origem.\n",
    "\n",
    "Na nossa rede o bias faz um papel similar! Nos dando liberdade para mover a função que estamos aprendendo, lembra que redes neurais são aproximadores de função? Pois é, literalmente!\n",
    "\n",
    "[Mais informações sobre bias (em inglês) nesse link](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks).\n",
    "\n",
    "\n",
    "**Importante**\n",
    "\n",
    "Nos diagramas que mostramos, o bias não era mostrado, por que? Geralmente pra deixar a visualização mais bonitinha não mostramos o biax, abaixo segue nossa rede reural com o bias no diagrama:\n",
    "\n",
    "![](images/bias_rede_neural.png) \n",
    "\n",
    "O bias é um nó na rede como qualquer outro, porém ele não se conecta aos nós da camada anterior, podemos pensar que seu valor é fixo e igual a 1.\n",
    "\n",
    ">*“Se hoje é o Dia da rede neural, ontem eu disse que rede neural... o dia da rede neural é dia da camada, do peso e das funções de ativação, mas também é o dia dos bias. Sempre que você olha uma camada densa, há sempre uma figura oculta, que é um bias atrás, o que é algo muito importante.”* - Marianne Monteiro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jutando os pedaços e codando uma Rede Neural\n",
    "\n",
    "Vamos codar a seguinte rede neural:\n",
    "\n",
    "![](images/codando_rede_neural.png)\n",
    "\n",
    "Nomeamos os neurônios pra ficar mais simples de identificá-los no código.\n",
    "\n",
    "    * ni_cj = i-ésimo neuronio da j-ésima camada\n",
    "\n",
    "\n",
    "> Os valores escolhidos são arbitrários e a saída não significa nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.00099999999999\n"
     ]
    }
   ],
   "source": [
    "# ------- Camada de entrada -------\n",
    "# Definimos esse valor! Na prática irá vir de uma base de dados.\n",
    "entrada = -3\n",
    "\n",
    "# ------- Primeira camada ---------\n",
    "n1_c1 = liga_neuronios([entrada], [-4.3], 1, funcao_de_ativacao)\n",
    "n2_c1 = liga_neuronios([entrada], [-1.3], 2.2, funcao_de_ativacao)\n",
    "\n",
    "# ------- Segunda camada ---------\n",
    "n1_c2 = liga_neuronios([n1_c1, n2_c1], [0.2, 3.3], 2.7, funcao_de_ativacao)\n",
    "\n",
    "# ------- Camada de saída --------\n",
    "\n",
    "# Na última camada não usamos ReLU, lembra? Podemos simplesmente não usar\n",
    "# função de entrada, para tal é só usar uma função que não faz nada (função identidade)\n",
    "# essa função é também denominada como linear nos frameworks de DL.\n",
    "def faz_nada(x):\n",
    "    return x\n",
    "\n",
    "saida = liga_neuronios([n1_c2], [4.1], -10, faz_nada)\n",
    "\n",
    "print(saida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grande sacada é das redes neurais é a definição de camadas e como elas se comunicam. Uma rede neural consiste basicamente da mesma computação baseada em camadas onde a entrada de uma camada é a camada imediamente anterior a ela.\n",
    "\n",
    "\n",
    "Ou seja: uma camada aprende a partir das camadas anteriores! \n",
    "\n",
    "\n",
    "Isso permite que a rede neural busque modificar os pesos de uma camada de maneira a gerar representações melhores para que a próxima camada tenha acesso a informações mais relevantes pra resolver o problema!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deixando o código eficiente, bonito e cheiroso\n",
    "\n",
    "Como a boa programadora que você é, deve ter notado que o código anterior é bastante ineficiente!!! \n",
    "\n",
    "**PERGUNTA: mais espeficamente para cada nó em uma rede neural quantas operações são necessárias para calcular sua saída?**\n",
    "\n",
    "Pense a respeito, a resposta segue abaixo...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular a saída desse nó precisamos cacular a soma ponderada de todos os nós da camada anterior, digamos que a camada anterior tem N nós.\n",
    "\n",
    "Então precisaríamos de O(N) operações para calcular a saída de um único nó nessa camada.\n",
    "\n",
    "Então por exemplo se tivermos 1 camada de entrada com 100 nós e 2 camadas intermediárias com 200 nós cada, precisaríamos realizaríamos o seguinte número de operações em cada camada:\n",
    "\n",
    "* camada intermediária 1: 100 * 200 (100 entradas para cada nó, 200 nós) = 20000\n",
    "* camada intermediária 2: 200 * 200 (200 entradas para cada nó, 200 nós) = 40000\n",
    "\n",
    "Totalizando: 60000 operações para calcular todas as saídas.\n",
    "\n",
    "Isso é péssimo :((! Queremos várias camadas na nossa rede, não podemos ser tão lentos :((\n",
    "\n",
    "Felizmente temos uma solução para deixar esse cálculo mais eficiente!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/freeze/max/1000/1*W8c9Eg-rJgLVhPmx4TD3Rw.jpeg?q=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que não sabemos qual o melhor `f` possível, e poderíamos tentar várias funções para se **ajustar** aos dados. Para isso modificamos os pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ⚠️ **Cuidado: não confunda parâmetro com hiperparâmetros!** Parâmetros são o que a sua rede usa para aprender (pesos e bias), enquanto hiperparâmetros são o que você define acerca da sua rede (número de camadas, qtde. de neurônios por camada, função de ativação de cada camada, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em busca da função perfeita\n",
    "\n",
    "\n",
    "### Exemplo: [base dados Iris](https://mari-linhares.github.io/codando-deep-learning/notebooks/glossario.html#iris)\n",
    "\n",
    "Digamos que para um exemplo da base de dados queremos determinar qual a espécie dessa planta.\n",
    "\n",
    "**Entradas**\n",
    "\n",
    "A base de dados iris tem 4 **[atributos](https://mari-linhares.github.io/codando-deep-learning/notebooks/glossario.html#atributos)** de uma planta que iremos usar como entrada.\n",
    "\n",
    "**Saídas**\n",
    "\n",
    "Neste caso a saída que nos interessa é a espécie da planta. Então digamos que a saída é um número que indica qual a espécie:\n",
    "\n",
    "0 = Iris Setosa , 1 = Iris Versicolour, 2 = Iris Virginica\n",
    "\n",
    "### Obtendo a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de exemplos na base: 150\n",
      "Atributos da primeira planta: [-0.555556  0.25     -0.864407 -0.916667]\n",
      "Especie da primeira planta: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marianne/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/marianne/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Trabalhar com os dados\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Atributos: iris_dataset.data\n",
    "# Espécie: iris_dataset.target\n",
    "iris_dataset = fetch_mldata('iris')\n",
    "\n",
    "print('Numero de exemplos na base:', len(iris_dataset.data))\n",
    "print('Atributos da primeira planta:', iris_dataset.data[0])\n",
    "print('Especie da primeira planta:', iris_dataset.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos codar uma função que resolve esse problema!\n",
    "\n",
    "Uma função para resolver esse problema precisa receber 4 parâmetros (cada um dos atributos da planta) e produzir uma saída (espécie da planta).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 1\n",
      "0 3\n"
     ]
    }
   ],
   "source": [
    "def f(x1, x2, x3, x4):\n",
    "    # Não importa os atributos pra mim a resposta é sempre: Setosa!!!!\n",
    "    return 0\n",
    "\n",
    "print(f(*iris_dataset.data[0]), iris_dataset.target[0])\n",
    "print(f(*iris_dataset.data[1]), iris_dataset.target[1])\n",
    "print(f(*iris_dataset.data[-1]), iris_dataset.target[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função acima é válida para este problema (4 entradas, 1 saída), o problema dela é que... Ela não ta ajudando a gente no nosso problema em nada! Ela simplesmente ignora os atributos e nos diz que qualquer exemplo é da espécie Setosa.\n",
    "\n",
    "Como podemos avaliar o quão boa é essa função? Uma métrica possível é [acurácia](https://mari-linhares.github.io/codando-deep-learning/notebooks/glossario.html#acuracia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Por exemplo: dado um banco com a altura de determinadas pessoas (entrada), queremos estimar o \"peso\" dessas pessoas. Nesse caso, o \"peso\" das pessoas é a variável que queremos estimar. Portanto, o \"peso\" nesse caso representaria a nossa saída. Sempre que a nossa saída é conhecida, nós dizemos que esse tipo de problema é um problema de **Aprendizagem Supervisionada**.Há casos em que não necessariamente o nosso problema tem uma saída explícita. Nesse caso, teremos uma **Aprendizagem Não-Supervisionada**.\n",
    "\n",
    "Além disso, quando a **saída assume qualquer valor real** (0, 1.2, 3.14, -26, +34, ...), nós dizemos que temos um **Problema de Regressão**. Por outro lado, quando a **saída é discreta** (0/1, saudável/doente, cachorro/gato/passarinho), nós temos **Problemas de Classificação**.\n",
    "\n",
    "\n",
    "\n",
    "A grande sacada é o que acontece dentro de `f`! A ideia é que não sabemos qual o melhor `f` possível, e poderíamos tentar várias funções para se **ajustar** aos dados.\n",
    "\n",
    "Em geral, elas são matrizes $NxD$, onde $N$ (#linhas) **representa o número de amostras** que seu banco de dados tem e $D$ (#colunas) **representa a quantidade de atributos** de cada amostra, também conhecida por *dimensionalidade*. Como exemplo, imagine que tenhamos um banco de dados com 1.000 amostras e cada amostra tem 5 atributos. Logo, nossas entradas seriam representadas por uma matriz $1000x5$, sacou? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As entradas são representadas pelas amostras dos seus dados. Em geral, elas são matrizes $NxD$, onde $N$ (#linhas) **representa o número de amostras** que seu banco de dados tem e $D$ (#colunas) **representa a quantidade de atributos** de cada amostra, também conhecida por *dimensionalidade*. Como exemplo, imagine que tenhamos um banco de dados com 1.000 amostras e cada amostra tem 5 atributos. Logo, nossas entradas seriam representadas por uma matriz $1000x5$, sacou? \n",
    "\n",
    "**As saídas, por sua vez, representam o que você quer que a sua rede aprenda**. Por exemplo: dado um banco com a altura de determinadas pessoas (entrada), queremos estimar o \"peso\" dessas pessoas. Nesse caso, o \"peso\" das pessoas é a variável que queremos estimar. Portanto, o \"peso\" nesse caso representaria a nossa saída. Sempre que a nossa saída é conhecida, nós dizemos que esse tipo de problema é um problema de **Aprendizagem Supervisionada**. Há casos em que não necessariamente o nosso problema tem uma saída explícita. Nesse caso, teremos uma **Aprendizagem Não-Supervisionada**. Além disso, quando a **saída assume qualquer valor real** (0, 1.2, 3.14, -26, +34, ...), nós dizemos que temos um **Problema de Regressão**. Por outro lado, quando a **saída é discreta** (0/1, homem/mulher, cachorro/gato/passarinho), nós temos **Problemas de Classificação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, weights=0.5):\n",
    "        self._weights = weights\n",
    "    \n",
    "    def function(self, _input):\n",
    "        return self._activation_function(_input * self._weights)\n",
    "    \n",
    "    def _activation_function(self, data):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyFirstNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Este conteúdo é baseado nos seguintes materiais:\n",
    "\n",
    "- [Capítulo 3](https://github.com/iamtrask/Grokking-Deep-Learning) de Grokking Deep Learning.\n",
    "- [Perceptron](https://en.wikipedia.org/wiki/Perceptron) da Wikipedia\n",
    "- [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) da Wikipedia\n",
    "- [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) da Wikipedia\n",
    "- [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) da Wikipedia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
