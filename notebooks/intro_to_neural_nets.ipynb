{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introdu√ß√£o a Redes Neurais: O que s√£o?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hist√≥ria das Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O come√ßo de tudo\n",
    "\n",
    "A hist√≥ria das redes neurais √© curiosa, engra√ßada e triste ao mesmo tempo. Ent√£o, senta que l√° vem hist√≥ria...\n",
    "\n",
    "Certo dia, em 1958, um certo cientista nova-yorkino, chamado [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt), inspirado pelo trabalho de seus colegas que estudavam sobre o c√©rebro humano, terminou de projetar um algoritmo que conseguia aprender sozinho a resolver problemas simples. Nascia ali, o **Perceptron** (que veremos mais adiante no curso).\n",
    "\n",
    "A ideia inicial do Perceptron, na verdade, era ser uma m√°quina ao inv√©s de um programa. E foi o que aconteceu. Utilizando o poderos√≠ssimo [IBM 704](https://en.wikipedia.org/wiki/IBM_704) que fazia 12 mil adi√ß√µes por segundo (s√≥ para voc√™ ter uma no√ß√£o, um iPhone 7 faz 300 bilh√µes) os cientistas constru√≠ram uma m√°quina denominada **Mark I Perceptron**.\n",
    "\n",
    "<img align='center' src='https://github.com/mari-linhares/codando-deep-learning/blob/master/notebooks/images/mark_i.jpg?raw=true' width=600>\n",
    "\n",
    "**A ideia do Mark I Perceptron era que, para uma dada imagem de entrada, a m√°quina deveria aprender sozinha a classificar a sa√≠da em 0 ou 1**. Mas, calma, nem imagens existiam naquela √©poca! Ent√£o, como projetaram isso? Os cientistas na √©poca conectaram 400 foto-c√©lulas a entrada do Mark I para simular uma imagem 20x20 (um pouco maior talvez do que as letras desse texto). E para permitir a calibra√ß√£o da m√°quina, cada entrada dessa era conectada a um potenci√¥metro que eram ajustados automaticamente por motores el√©tricos para mapear as entradas na sa√≠da final 0 ou 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The winter is coming... üòî\n",
    "\n",
    "O Perceptron, na √©poca, foi um sucesso! Afinal de contas, **ele foi um dos primeiros algoritmos capazes de aprender sozinho**. Al√©m disso, j√° foi provado que **o Perceptron tem garantia de sucesso quando as duas classes s√£o linearmente separ√°veis**. Por√©m, √© a√≠ que est√° o problema: o Perceptron nada mais √© que um classificador bin√°rio linear. Ou seja, ele s√≥ funciona quando o seu problema √© bin√°rio (2 classes) e seus dados podem ser separados por uma simples reta. Essas condi√ß√µes, no mundo real, s√£o muito dif√≠ceis de acontecer. S√≥ para exemplificar, como voc√™ separia os dados abaixo com apenas uma reta?\n",
    "\n",
    "<img align='center' src='https://jarvmiller.github.io/post/2017-10-20-neural-networks-units-and-decision-boundaries_files/figure-html/xor%20setup-1.png' width=400>\n",
    "\n",
    "[Fonte da imagem](https://jarvmiller.github.io/2017/10/14/neural-nets-pt1/)\n",
    "\n",
    "Pois √©. O Perceptron tamb√©m n√£o consegue resolver esse simples problema (para quem n√£o reparou, o gr√°fico acima representa a [porta XOR](https://pt.wikipedia.org/wiki/Porta_XOR)). Na √©poca, isso desanimou tanto os estudiosos da √°rea de Intelig√™ncia Artificial, que pesquisas nessa √°rea s√≥ foram retomadas de fato na d√©cada de 80. Tal per√≠odo ficou conhecido como o **inverno da Intelig√™ncia Artificial** (*AI Winter*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O salvador da p√°tria: Geoffrey Hinton\n",
    "\n",
    "O maior problema do Perceptron era que ele era apenas um s√≥. Intuitivamente, √© f√°cil perceber que um neur√¥nio s√≥ n√£o faz uma rede neural ~~(assim como uma andorinha s√≥ n√£o faz ver√£o)~~. Pensando assim, muita gente tentou colocar um monte de Perceptrons conectados entre si para tentar resolver um problema. Por√©m, muitos do que fizeram isso se depararam com um problema: **como propagar o erro da sa√≠da para as entradas?** Em outras palavras, **como fazer esse monte de Perceptrons aprenderem ao mesmo tempo sem que um atrapalhe o que o outro aprendeu?\"**.\n",
    "\n",
    "Pensando nisso, o famoso [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) desenvolveu o algoritmo **[backpropagation](https://en.wikipedia.org/wiki/Backpropagation)** na d√©cada de 80. Utilizando o conceito de gradientes e regra da cadeia, tal algoritmo pega o erro de uma rede neural e propaga-o at√© as entradas, fazendo leves ajustes nos par√¢metros (pesos) da rede. Com isso, Redes Neurais com mais de um neur√¥nio e mais de uma camada poderiam come√ßar a ser desenvolvidas e treinadas em problemas mais complexos. Como eu disse, *poderiam*...\n",
    "\n",
    "O problema era que na d√©cada de 80, e mesmo na d√©cada de 90, o treinamento de tais redes e aplica√ß√£o do backpropagation era muito pesado ainda. Mesmo supercomputadores se matavam para treinar e executar tais redes ainda. Ent√£o, o que fazer?\n",
    "\n",
    "### Obrigado, gamers!\n",
    "\n",
    "Mais uma vez o mundo foi salvo gra√ßas aos **gamers**. Isso mesmo. Essa obsess√£o dos gamers em sempre querer computadores mais potentes e jogos com gr√°ficos cada vez melhores, fez com que a ind√∫stria dos computadores, especialmente a das GPUs se desenvolvessem num ritmo assustador - regido pela Lei de Murphy. Mas, o que os jogos t√™m a ver com o desenvolvimento das Redes Neurais? \n",
    "\n",
    "A resposta √©: matrizes! Como vamos ver num dos pr√≥ximos assuntos, **Redes Neurais tem tudo a ver com matrizes**. Basicamente, Redes Neurais fazem um monte de c√°lculo sobre matrizes, como: soma, multiplic√£o, opera√ß√µe ponto-a-ponto, etc... E, como GPUs s√£o computadores especializados em c√°lculos sobre matrizes, o campo das Redes Neurais pode se desenvolver como nunca. Cada vez mais, redes mais complexas e pesadas puderam ser desenvolvidas e treinadas.\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "Agora que j√° tivemos uma introdu√ß√£o sobre a hist√≥ria das Redes Neurais, chegou a hora de aprendermos mais sobre elas. Ent√£o, prepara um caf√© que chegou a hora de come√ßar os estudos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdu√ß√£o √† Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais podem ser vistas de maneira bastante simplificada como **fun√ß√µes matem√°ticas**. Uma simples defini√ß√£o de redes neurais √©:\n",
    "\n",
    "> \"Redes Neurais s√£o *aproximadores de fun√ß√£o*\"\n",
    "\n",
    "Desse modo, similar √† fun√ß√µes, as redes neurais possuem entradas e sa√≠das."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, weights=0.5):\n",
    "        self._weights = weights\n",
    "    \n",
    "    def function(self, _input):\n",
    "        return self._activation_function(_input * self._weights)\n",
    "    \n",
    "    def _activation_function(self, data):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyFirstNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer√™ncias\n",
    "\n",
    "Este conte√∫do √© baseado nos seguintes materiais:\n",
    "\n",
    "- [Cap√≠tulo 3](https://github.com/iamtrask/Grokking-Deep-Learning) de Grokking Deep Learning.\n",
    "- [Perceptron](https://en.wikipedia.org/wiki/Perceptron) da Wikipedia\n",
    "- [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) da Wikipedia\n",
    "- [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) da Wikipedia\n",
    "- [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) da Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
