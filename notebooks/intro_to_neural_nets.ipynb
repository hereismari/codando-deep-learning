{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introdu√ß√£o a Redes Neurais: O que s√£o?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hist√≥ria das Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hist√≥ria das redes neurais √© curiosa, engra√ßada e triste ao mesmo tempo. \n",
    "\n",
    "Certo dia, em 1958, um certo cientista nova-yorkino, chamado [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt), inspirado pelo trabalho de seus colegas que estudavam sobre o c√©rebro humano, terminou de projetar um algoritmo que conseguia aprender sozinho a resolver problemas simples. Nascia ali, o **Perceptron** (que veremos mais adiante no curso).\n",
    "\n",
    "A ideia inicial do Perceptron, na verdade, era ser uma m√°quina ao inv√©s de um programa. E foi o que aconteceu. Utilizando o poderos√≠ssimo [IBM 704](https://en.wikipedia.org/wiki/IBM_704) que fazia 12 mil adi√ß√µes por segundo (s√≥ para voc√™ ter uma no√ß√£o, um iPhone 7 faz 300 bilh√µes) os cientistas constru√≠ram uma m√°quina denominada **Mark I Perceptron**.\n",
    "\n",
    "<img align='center' src='https://github.com/mari-linhares/codando-deep-learning/blob/master/notebooks/images/mark_i.jpg?raw=true' width=600>\n",
    "\n",
    "**A ideia do Mark I Perceptron era que, para uma dada imagem de entrada, a m√°quina deveria aprender sozinha a classificar a sa√≠da em 0 ou 1**. Mas, calma, nem imagens existiam naquela √©poca! Ent√£o, como projetaram isso? Os cientistas na √©poca conectaram 400 foto-c√©lulas a entrada do Mark I para simular uma imagem 20x20 (um pouco maior talvez do que as letras desse texto). E para permitir a calibra√ß√£o da m√°quina, cada entrada dessa era conectada a um potenci√¥metro que eram ajustados automaticamente por motores el√©tricos para mapear as entradas na sa√≠da final 0 ou 1. \n",
    "\n",
    "O Perceptron, na √©poca, foi um sucesso! Afinal de contas, **ele foi um dos primeiros algoritmos capazes de aprender sozinho**. Al√©m disso, j√° foi provado que **o Perceptron tem garantia de sucesso quando as duas classes s√£o linearmente separ√°veis**. Por√©m, √© a√≠ que est√° o problema: o Perceptron nada mais √© que um classificador bin√°rio linear. Ou seja, ele s√≥ funciona quando o seu problema √© bin√°rio (2 classes) e seus dados podem ser separados por uma simples reta. Essas condi√ß√µes, no mundo real, s√£o muito dif√≠ceis de acontecer. S√≥ para exemplificar, como voc√™ separia os dados abaixo com apenas uma reta?\n",
    "\n",
    "<img align='center' src='https://jarvmiller.github.io/post/2017-10-20-neural-networks-units-and-decision-boundaries_files/figure-html/xor%20setup-1.png' width=400>\n",
    "\n",
    "[Fonte da imagem](https://jarvmiller.github.io/2017/10/14/neural-nets-pt1/)\n",
    "\n",
    "Pois √©. O Perceptron tamb√©m n√£o consegue resolver esse simples problema (para quem n√£o reparou, o gr√°fico acima representa a [porta XOR](https://pt.wikipedia.org/wiki/Porta_XOR)) üòî. Na √©poca, isso desanimou tanto os estudiosos da √°rea de Intelig√™ncia Artificial, que pesquisas nessa √°rea s√≥ foram retomadas de fato na d√©cada de 80. Tal per√≠odo ficou conhecido como o **inverno da Intelig√™ncia Artificial** (*AI Winter*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdu√ß√£o √† Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais podem ser vistas de maneira bastante simplificada como **fun√ß√µes matem√°ticas**. Uma simples defini√ß√£o de redes neurais √©:\n",
    "\n",
    "> \"Redes Neurais s√£o *aproximadores de fun√ß√£o*\"\n",
    "\n",
    "Desse modo, similar √† fun√ß√µes, as redes neurais possuem entradas e sa√≠das."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, weights=0.5):\n",
    "        self._weights = weights\n",
    "    \n",
    "    def function(self, _input):\n",
    "        return self._activation_function(_input * self._weights)\n",
    "    \n",
    "    def _activation_function(self, data):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyFirstNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer√™ncias\n",
    "\n",
    "Este conte√∫do √© baseado nos seguintes materiais:\n",
    "\n",
    "- [Cap√≠tulo 3](https://github.com/iamtrask/Grokking-Deep-Learning) de Grokking Deep Learning.\n",
    "- [Perceptron](https://en.wikipedia.org/wiki/Perceptron) da Wikipedia\n",
    "- [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) da Wikipedia\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
