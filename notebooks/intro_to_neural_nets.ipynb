{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "# Matemática\n",
    "import math\n",
    "# Manipulação de Vetores\n",
    "import numpy as np\n",
    "\n",
    "# Fixar números aleatórios gerados\n",
    "np.random.seed(0)\n",
    "\n",
    "# Trabalhar com dados\n",
    "import pandas as pd\n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "\n",
    "# Utilidades\n",
    "import utils\n",
    "\n",
    "# Recarregar automaticamente dependências caso elas mudem\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução à Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais podem ser vistas de maneira bastante simplificada como **funções matemáticas**. Uma simples definição de redes neurais é:\n",
    "\n",
    "> \"Redes Neurais são *aproximadores de função*\"\n",
    "\n",
    "Desse modo, similar à funções, as redes neurais possuem entradas e saídas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falando em Deep Learning, essas entradas são normalmente **tensores**, isto é, vetores de diferentes dimensões. De maneira matemática, você pode pensar em tensores da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.kdnuggets.com/wp-content/uploads/scalar-vector-matrix-tensor.jpg)\n",
    "Imagem de: https://www.kdnuggets.com/2018/05/wtf-tensor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já os matemáticos que gostam de cachorros, preferem pensar em tensores da seguinte forma:\n",
    "\n",
    "![](https://pbs.twimg.com/media/CvUaME-VIAACHLb.jpg)\n",
    "Imagem de: https://pbs.twimg.com/media/CvUaME-VIAACHLb.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, um **scalar** é apenas um valor (0-dimensional); um **vetor** é um array de scalares (uni-dimensional); uma **matriz** é um array de vetores (bi-dimensional); e um **tensor** é qualquer vetor com N-dimensões (por exemplo, um cubo é um tensor de 3 dimensões). Vale salientar que, tecnicamente, o conceito de tensor engloba todos eles. Porém, na prática, chamamos de tensores os vetores com 3 ou mais dimensões. Veja [**esse vídeo**](https://www.youtube.com/watch?v=f5liqUk0ZTw) para mais detalhes.\n",
    "\n",
    "Ainda pensando dessa forma, poderíamos imaginar um tensor 4-dimensional como um **array de cubos**; um tensor 5-dimensional seria uma **matriz de cubos**; um tensor 6-dimensional seria um **cubo de cubos**, e assim por diante...\n",
    "\n",
    "<img align='center' src='https://media3.giphy.com/media/xT0xeJpnrWC4XWblEk/giphy.gif'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# História das Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A história das redes neurais é curiosa, engraçada e triste ao mesmo tempo. \n",
    "\n",
    "Certo dia, em 1958, um certo cientista nova-yorkino, inspirado pelo trabalho de seus colegas que estudavam sobre o cérebro humano, terminou de projetar um algoritmo que conseguia aprender sozinho a resolver problemas simples. Nascia ali, o **Perceptron**. Em sua estrutura básica, o Perceptron é formado por 3 partes:\n",
    "\n",
    "<img align='center' src='https://github.com/arnaldog12/Manual-Pratico-Deep-Learning/raw/2d4adc1a095b815d523cf1d84ba3422c8c30ae1d/images/perceptron.png' width=400>\n",
    "\n",
    "- __entradas__ $x_1,...,x_D$: representam os atributos dos seus dados com dimensionalidade $D$. O Perceptron aceita qualquer tamanho de entrada, porém a saída é sempre apenas um valor.\n",
    "- __junção aditiva__ $\\sum$: também chamada de _função agregadora_, nada mais é que a soma ponderada das entradas com os __pesos__ ($w_1,...,w_D)$. Em geral, o resultado é somado com um __bias__ $b$, responsável por deslocar o resultado do somatório. A junção aditiva é descrita pela seguinte fórmula:\n",
    "\n",
    "$$\\sum_i^D{x_iw_i} + b$$\n",
    "\n",
    "- __função de ativação__ $f$: utilizada para mapear o resultado da junção aditiva em uma saída esperada. No caso do Perceptron, ela é a função _step_ (também conhecida como _função degrau_) para mapear a saída em um valor discreto (0 ou 1):\n",
    "\n",
    "$$f = \\begin{cases}1 & se \\ wx+b > 0\\\\0 & caso \\ contr\\acute ario\\end{cases}$$\n",
    "\n",
    "A ideia inicial do Perceptron, na verdade, era ser uma máquina ao invés de um programa. E foi o que aconteceu. Utilizando o poderosíssimo [IBM 704](https://en.wikipedia.org/wiki/IBM_704) que fazia 12 mil adições por segundo (só para você ter uma noção, um iPhone 7 faz 300 bilhões) os caras construíram uma máquina denominada **Mark I Perceptron**.\n",
    "\n",
    "<img align='center' src='images/mark_i.jpg' width=600>\n",
    "\n",
    "A ideia do Mark I Perceptron era que, para uma dada imagem de entrada, a máquina deveria aprender sozinha a classificar a saída em 0 ou 1. Mas, calma, nem imagens existiam naquela época! Então, como projetaram isso? Os cientistas na época conectaram 400 foto-células a entrada do Mark I para simular uma imagem 20x20 (um pouco maior talvez do que as letras desse texto). E para permitir a calibração da máquina, cada entrada dessa era conectada a um potenciômetro (que simulavam os pesos) que eram ajustados automaticamente por motores elétricos para mapear as entradas na saída final 0 ou 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, weights=0.5):\n",
    "        self._weights = weights\n",
    "    \n",
    "    def function(self, _input):\n",
    "        return self._activation_function(_input * self._weights)\n",
    "    \n",
    "    def _activation_function(self, data):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyFirstNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Este conteúdo é baseado nos seguintes materiais:\n",
    "\n",
    "* [Capítulo 3](https://github.com/iamtrask/Grokking-Deep-Learning) de Grokking Deep Learning.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
