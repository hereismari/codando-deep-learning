{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# História das Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A história das redes neurais é curiosa, engraçada e triste ao mesmo tempo. \n",
    "\n",
    "Certo dia, em 1958, um certo cientista nova-yorkino, inspirado pelo trabalho de seus colegas que estudavam sobre o cérebro humano, terminou de projetar um algoritmo que conseguia aprender sozinho a resolver problemas simples. Nascia ali, o **Perceptron** (que veremos mais adiante no curso).\n",
    "\n",
    "A ideia inicial do Perceptron, na verdade, era ser uma máquina ao invés de um programa. E foi o que aconteceu. Utilizando o poderosíssimo [IBM 704](https://en.wikipedia.org/wiki/IBM_704) que fazia 12 mil adições por segundo (só para você ter uma noção, um iPhone 7 faz 300 bilhões) os cientistas construíram uma máquina denominada **Mark I Perceptron**.\n",
    "\n",
    "<img align='center' src='https://github.com/mari-linhares/codando-deep-learning/blob/master/notebooks/images/mark_i.jpg?raw=true' width=600>\n",
    "\n",
    "**A ideia do Mark I Perceptron era que, para uma dada imagem de entrada, a máquina deveria aprender sozinha a classificar a saída em 0 ou 1**. Mas, calma, nem imagens existiam naquela época! Então, como projetaram isso? Os cientistas na época conectaram 400 foto-células a entrada do Mark I para simular uma imagem 20x20 (um pouco maior talvez do que as letras desse texto). E para permitir a calibração da máquina, cada entrada dessa era conectada a um potenciômetro que eram ajustados automaticamente por motores elétricos para mapear as entradas na saída final 0 ou 1. \n",
    "\n",
    "O Perceptron, na época, foi um sucesso! Afinal de contas, **ele foi um dos primeiros algoritmos capazes de aprender sozinho**. Além disso, já foi provado que **o Perceptron tem garantia de sucesso quando as duas classes são linearmente separáveis**. Porém, é aí que está o problema: o Perceptron nada mais é que um classificador binário linear. Ou seja, ele só funciona quando o seu problema é binário (2 classes) e seus dados podem ser separados por uma simples reta. Essas condições, no mundo real, são muito difíceis de acontecer. Só para exemplificar, como você separia os dados abaixo com apenas uma reta?\n",
    "\n",
    "<img align='center' src='https://jarvmiller.github.io/post/2017-10-20-neural-networks-units-and-decision-boundaries_files/figure-html/xor%20setup-1.png' width=400>\n",
    "\n",
    "[Fonte](https://jarvmiller.github.io/2017/10/14/neural-nets-pt1/)\n",
    "\n",
    "Pois é. O Perceptron também não consegue resolver esse simples problema (para quem não reparou, o gráfico acima representa a [porta XOR](https://pt.wikipedia.org/wiki/Porta_XOR)). Na época, isso desanimou tanto os estudiosos da área de Inteligência Artificial, que pesquisas nessa área só foram retomadas de fato na década de 80. Tal período ficou conhecido como o **inverno da Inteligência Artificial** (*AI Winter*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução à Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais podem ser vistas de maneira bastante simplificada como **funções matemáticas**. Uma simples definição de redes neurais é:\n",
    "\n",
    "> \"Redes Neurais são *aproximadores de função*\"\n",
    "\n",
    "Desse modo, similar à funções, as redes neurais possuem entradas e saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, weights=0.5):\n",
    "        self._weights = weights\n",
    "    \n",
    "    def function(self, _input):\n",
    "        return self._activation_function(_input * self._weights)\n",
    "    \n",
    "    def _activation_function(self, data):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyFirstNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Este conteúdo é baseado nos seguintes materiais:\n",
    "\n",
    "* [Capítulo 3](https://github.com/iamtrask/Grokking-Deep-Learning) de Grokking Deep Learning.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
