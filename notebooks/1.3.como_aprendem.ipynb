{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Introdução a Redes Neurais: como aprendem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matemática + manipulação de vetores\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# \"Fixar\" números aleatórios a serem gerados\n",
    "np.random.seed(0)\n",
    "\n",
    "# Trabalhar com os dados\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utilidades\n",
    "import utils\n",
    "\n",
    "# Recarregar automaticamente dependências caso elas mudem\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando dados sintéticos\n",
    "\n",
    "$Y = 7 * X + 15$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de dados sintéticos gerados\n",
    "SYNT_TRAIN_SIZE = 200\n",
    "# controla o quão espalhados são os dados\n",
    "STD_DEV = 0.7\n",
    "\n",
    "def random_error(size, mu=0, std_dev=0.5):\n",
    "    return np.random.normal(mu, std_dev, size)\n",
    "\n",
    "def add_batch_dim(tensor):\n",
    "    if len(tensor.shape) == 1:\n",
    "        return np.expand_dims(tensor, axis=1)\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def remove_batch_dim(tensor):\n",
    "    return np.squeeze(tensor, axis=1)\n",
    "    \n",
    "def generate_x(size, use_batch_dim=True):\n",
    "    x = np.random.rand(size)\n",
    "    if use_batch_dim:\n",
    "        x = add_batch_dim(x)\n",
    "    return x\n",
    "\n",
    "def plot_line(x, y, style='-b'):\n",
    "    x, y = remove_batch_dim(x), remove_batch_dim(y)\n",
    "    return plt.plot([min(x), max(x)], [min(y), max(y)], style)\n",
    "\n",
    "def generate_f(x, a=7, b=15, error_std_dev=0.5, use_batch_dim=True):\n",
    "    y = a * x + b + random_error(x.shape, std_dev=error_std_dev)\n",
    "    if use_batch_dim:\n",
    "        y = add_batch_dim(y)\n",
    "    return y\n",
    "\n",
    "# gera valores aleatórios para x\n",
    "synt_x = generate_x(SYNT_TRAIN_SIZE)\n",
    "# gera a funcão: Y = 7 * X + 15\n",
    "synt_y = generate_f(synt_x, error_std_dev=STD_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(synt_x, synt_y, 'ro', alpha=0.4)\n",
    "plot_line(synt_x, synt_x * 7 + 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers=[1], input_size=1, activations=[None]):\n",
    "        assert len(layers) == len(activations)\n",
    "        self.input_size = input_size\n",
    "        self.layers = layers\n",
    "        self.activations, self._act_devs = self.get_act(activations)\n",
    "        \n",
    "        self.weights, self.biases = self.define_params()\n",
    "        self._current_batch = []\n",
    "        \n",
    "    def get_act(self, act_names):\n",
    "        def _no_act(x):\n",
    "            return x\n",
    "        def _dev_no_act(x):\n",
    "            return np.ones(x.shape)\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        def _dev_sigmoid(x):\n",
    "            return x * (1 - x)\n",
    "        \n",
    "        def _relu(x):\n",
    "            return np.maximum(1e-15, x)\n",
    "        \n",
    "        def _dev_relu(x):\n",
    "            return (x > 0) * 1.0\n",
    "        \n",
    "        activations = []\n",
    "        act_devs = []\n",
    "        for act_name in act_names:\n",
    "            if act_name is None:\n",
    "                act, dev_act = _no_act, _dev_no_act\n",
    "            elif act_name == 'sigmoid':\n",
    "                act, dev_act = _sigmoid, _dev_sigmoid\n",
    "            elif act_name == 'relu':\n",
    "                act, dev_act = _relu, _dev_relu\n",
    "            else:\n",
    "                raise ValueError('Activation function is not valid: %s' % act_name)\n",
    "            \n",
    "            activations.append(act)\n",
    "            act_devs.append(dev_act)\n",
    "        return activations, act_devs\n",
    "    \n",
    "\n",
    "    def define_params(self):\n",
    "        '''He-et-all initialization'''\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip([self.input_size] + self.layers, self.layers)):\n",
    "            weights.append(np.random.randn(in_dim, out_dim) * np.sqrt(2/in_dim))\n",
    "            biases.append(np.random.randn(out_dim) * np.sqrt(2/in_dim))           \n",
    "            print('Weight %d shape =' % i, weights[i].shape)\n",
    "            print('Bias %d shape =' % i, biases[i].shape)\n",
    "            \n",
    "        return weights, biases\n",
    "\n",
    "\n",
    "    def update_params(self, gradients, learning_rate=0.1):\n",
    "        assert len(gradients) == len(self.weights), (len(gradients), len(self.weights))\n",
    "        assert len(gradients) == len(self.biases), (len(gradients), len(self.biases))\n",
    "        \n",
    "        for i, grad in enumerate(gradients[::-1]):\n",
    "            assert grad['weights'].shape == self.weights[i].shape\n",
    "            self.weights[i] -= learning_rate * grad['weights']\n",
    "            self.biases[i] -= learning_rate * grad['biases']\n",
    "\n",
    "    \n",
    "    def run_batch(self, batch):\n",
    "        self._current_batch = [batch]\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            output = np.dot(self._current_batch[-1], w) + b\n",
    "            output = self.activations[i](output)\n",
    "            self._current_batch.append(output)\n",
    "        \n",
    "        self._current_batch = self._current_batch[::-1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, learning_rate = 0.01, loss_name='l2',\n",
    "                 print_mod=1000, verbose=True):\n",
    "        \n",
    "        def _accuracy(pred_y, real_y):\n",
    "            print(pred_y, real_y)\n",
    "            p = np.argmax(self.softmax(pred_y), axis=1)\n",
    "            return np.sum(p == real_y) / len(pred_y)\n",
    "            \n",
    "        \n",
    "        self.model = model\n",
    "        self.loss_name = loss_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss, self.loss_dev = self._define_loss()\n",
    "        \n",
    "        self.train_step = 0\n",
    "        self.eval_steps = []\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.print_mod = print_mod\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "        \n",
    "        self._metrics = {\n",
    "            'accuracy': _accuracy\n",
    "        }\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x)\n",
    "        return (exps / np.sum(exps, axis=1, keepdims=True))\n",
    "        \n",
    "    def _define_loss(self):\n",
    "        def _l2(pred_y, real_y):\n",
    "            n = len(pred_y)\n",
    "            return (1.0/2) * (1.0/n) * np.sum(np.power(pred_y - real_y, 2))\n",
    "        \n",
    "        def _l2_dev(pred_y, real_y):\n",
    "            n = len(pred_y)\n",
    "            return (pred_y - real_y) * (1.0/n)\n",
    "        \n",
    "        def _cross_entropy(pred_y, real_y):\n",
    "            m = real_y.shape[0]\n",
    "            p = self.softmax(pred_y)\n",
    "            # We use multidimensional array indexing to extract \n",
    "            # softmax probability of the correct label for each sample.\n",
    "            # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing.\n",
    "            log_likelihood = -np.log(p[range(m), real_y.astype(int)])\n",
    "            loss = np.sum(log_likelihood) / m\n",
    "            return loss\n",
    "    \n",
    "        \n",
    "        def _cross_entropy_dev(pred_y, real_y):\n",
    "            m = real_y.shape[0]\n",
    "            grad = self.softmax(pred_y)\n",
    "            grad[range(m), real_y.astype(int)] -= 1\n",
    "            grad = grad / m\n",
    "            return grad\n",
    "\n",
    "        if self.loss_name == 'l2':\n",
    "            return _l2, _l2_dev\n",
    "        elif self.loss_name == 'cross-entropy':\n",
    "            return _cross_entropy, _cross_entropy_dev\n",
    "        else:\n",
    "            raise ValueError('Invalid loss name: %s' % self.loss_name)\n",
    "\n",
    "\n",
    "    def train(self, batch_x, batch_y):\n",
    "        self.train_step += 1\n",
    "        \n",
    "        # run feed forward network\n",
    "        pred_y = self.model.run_batch(batch_x)\n",
    "        # save loss\n",
    "        self.train_losses.append(self.loss(pred_y, batch_y))\n",
    "        # get gradients\n",
    "        grads = self.generate_gradients(pred_y, batch_y, batch_x)\n",
    "        # update parameters\n",
    "        self.model.update_params(grads, self.learning_rate)\n",
    "\n",
    "        if self.verbose and (self.train_step - 1) % self.print_mod == 0:\n",
    "            print('Loss: %.4f for step %d' % (self.train_losses[-1], self.train_step))\n",
    "\n",
    "\n",
    "    def eval(self, batch_x, batch_y, metrics=[]):\n",
    "        # run feed forward network\n",
    "        pred_y = self.model.run_batch(batch_x)\n",
    "        # loss\n",
    "        loss = self.loss(pred_y, batch_y)\n",
    "        self.eval_losses.append(loss)\n",
    "        # metrics\n",
    "        res_metrics = []\n",
    "        for m in metrics:\n",
    "            if m in self._metrics:\n",
    "                res_metrics.append(self._metrics[m](pred_y, batch_y))\n",
    "            else:\n",
    "                raise ValueError('Invalid metric: %s' % m)\n",
    "        \n",
    "        self.eval_steps.append(self.train_step)\n",
    "            \n",
    "        return loss, res_metrics\n",
    "\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        if len(self.eval_losses) > 0:\n",
    "            plt.title('Train Loss: %.4f | Test Loss: %.4f for step %d' % (self.train_losses[-1], self.eval_losses[-1], self.train_step))\n",
    "        else:\n",
    "            plt.title('Train Loss: %.4f for step %d' % (self.train_losses[-1], self.train_step))    \n",
    "        plt.plot([i for i in range(self.train_step)], self.train_losses)\n",
    "        plt.plot([i for i in self.eval_steps], self.eval_losses)\n",
    "        \n",
    "        \n",
    "    def generate_gradients(self, pred_y, real_y, data_x):\n",
    "        grad = []\n",
    "        input_size = pred_y.shape[0]\n",
    "        j = len(self.model.activations) - 1\n",
    "        k = len(self.model.weights) - 1\n",
    "        dly = self.loss_dev(pred_y, real_y) * self.model._act_devs[j](self.model._current_batch[0])\n",
    "        dlx = np.dot(dly, self.model.weights[k].T)\n",
    "\n",
    "        for i, (w, b) in enumerate(zip(self.model.weights[::-1], self.model.biases[::-1])):\n",
    "            dlw = np.dot(self.model._current_batch[i+1].T, dly)\n",
    "            dlb = np.sum(dly)\n",
    "            # print('weight:', w.shape, 'bias:', b.shape)\n",
    "            # print('dlw:', dlw.shape, 'dlb:', dlb.shape)\n",
    "            # print('dly:', dly.shape, 'dlx:', dlx.shape)\n",
    "            grad.append({\n",
    "                'weights': dlw,\n",
    "                'biases': dlb\n",
    "            })\n",
    "            \n",
    "            j -= 1\n",
    "            k -= 1\n",
    "            if i != len(self.model.weights)-1:\n",
    "                dly = dlx * self.model._act_devs[j](self.model._current_batch[i+1])\n",
    "                dlx = np.dot(dly, self.model.weights[k].T)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "##### L2 loss with 1 layer, no activation\n",
    "\n",
    "**Loss**\n",
    "\n",
    "$$L = 1/2 * 1/n * \\sum{(y_i - ŷ_i)^{2}}$$\n",
    "$$L = 1/2 * 1/n * \\sum{(y_i - w_i * x_i + b_i)^{2}}$$\n",
    "\n",
    "**Gradients**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i} = 1/2 * 1/n * 2 * \\sum{(y_i - ŷ_i)} * \\frac{\\partial {ŷ_i}}{\\partial w_i} $$\n",
    "$$\\frac{\\partial L}{\\partial w_i} = 1/n * \\sum{(y_i - ŷ_i)} * x_i$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_i} = 1/2 * 1/n * 2 * \\sum{(y_i - ŷ_i)} * \\frac{\\partial {ŷ_i}}{\\partial b_i} $$\n",
    "$$\\frac{\\partial L}{\\partial b_i} = 1/n * \\sum{(y_i - ŷ_i)} * 1$$\n",
    "\n",
    "\n",
    "##### L2 loss with 2 layers, relu activation in the hidden layer\n",
    "\n",
    "**Loss**\n",
    "\n",
    "$$L = 1/2 * 1/n * \\sum{(y_i - ŷ_i)^{2}}$$\n",
    "$$L = 1/2 * 1/n * \\sum{(y_i - (w_j * x_j + b_j))^{2}}$$\n",
    "$$x_j = relu(w_i * x_i + b_i)$$\n",
    "\n",
    "\n",
    "**Gradients**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i} = 1/n * \\sum{(y_i - ŷ_i)} * x_j $$\n",
    "$$\\frac{\\partial L}{\\partial b_i} = 1/n * \\sum{(y_i - ŷ_i)} * 1$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_j} = 1/n * \\sum{(y_i - ŷ_i)} * x_j * x_i, se relu() > 0$$\n",
    "$$\\frac{\\partial L}{\\partial b_j} = 1/n * \\sum{(y_i - ŷ_i)} * x_j, se relu() > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "t = Trainer(nn, verbose=False)\n",
    "for i in range(100000):\n",
    "    t.train(synt_x, synt_y)\n",
    "\n",
    "t.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparando com a realidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parâmetros aprendidos:')\n",
    "print('pesos:', nn.weights)\n",
    "print('bias:', nn.biases)\n",
    "print('Função que modela os dados: 7 * X + 15')\n",
    "plot_line(synt_x, nn.run_batch(synt_x), '--r')\n",
    "plot_line(synt_x, synt_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uma função um pouco mais complicada\n",
    "\n",
    "$Y = 7 * log(x) + 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_error(size, mu=0, std_dev=0.8):\n",
    "    return np.random.normal(mu, std_dev, size)\n",
    "\n",
    "synt_x = np.random.rand(SYNT_TRAIN_SIZE)\n",
    "synt_y = np.reshape(7 * np.log(synt_x) + 1 + get_random_error(SYNT_TRAIN_SIZE), (SYNT_TRAIN_SIZE, 1))\n",
    "\n",
    "synt_x = np.reshape(synt_x, (SYNT_TRAIN_SIZE, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(synt_x, synt_y, 'ro', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers=[10, 1], activations=['sigmoid', None])\n",
    "t = Trainer(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    t.train(synt_x, synt_y)\n",
    "\n",
    "t.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parâmetros aprendidos:')\n",
    "print('pesos:', nn.weights)\n",
    "print('bias:', nn.biases)\n",
    "print('Função que modela os dados: 7 * X + 15')\n",
    "plt.plot(synt_x, nn.run_batch(synt_x), 'or', alpha=0.3)\n",
    "plt.plot(synt_x, synt_y, 'og', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E se os dados forem não lineares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers=[10, 2], input_size=2, activations=['relu', None])\n",
    "t = Trainer(nn, verbose=False)\n",
    "for i in range(100000):\n",
    "    t.train(xor_x, xor_y)\n",
    "\n",
    "t.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xor_x, nn.run_batch(xor_x), 'bo', xor_x, xor_y, 'ro', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers=[10, 1], input_size=2, activations=[None, None])\n",
    "t = Trainer(nn, verbose=False)\n",
    "for i in range(100000):\n",
    "    t.train(xor_x, xor_y)\n",
    "\n",
    "t.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(xor_x, nn.run_batch(xor_x), 'bo', xor_x, xor_y, 'ro', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de dados Iris\n",
    "\n",
    "A base de dados Iris foi publicada originalmente no [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "Uma das bases de dados mais conhecidas. É uma pequena base contendo informações sobre plantas de 3 diferentes espécies (setosa, versicolour e virginica). É bastante utilizada para classificação das espécies\n",
    "\n",
    "* Atributos:\n",
    "    1. sepal length in cm \n",
    "    2. sepal width in cm \n",
    "    3. petal length in cm \n",
    "    4. petal width in cm \n",
    "5. Classes: \n",
    "    0. Iris Setosa \n",
    "    1. Iris Versicolour \n",
    "    2. Iris Virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar iris dataset\n",
    "iris = fetch_mldata('iris')\n",
    "# np.c_ concatena as features e targets do dataset\n",
    "iris_data = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                         columns=['x0', 'x1', 'x2', 'x3', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris_data.drop(['target'], axis=1).diff().hist(color='k', alpha=0.5, bins=10, figsize=(4, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.data\n",
    "y = iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(x, y, batch_size=True):\n",
    "    idx = np.random.permutation(len(x))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    \n",
    "    for i in range(0, len(x)-batch_size-1, batch_size):\n",
    "        batch_x = x[i:i+batch_size]\n",
    "        batch_y = y[i:i+batch_size]\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers=[10, 4], input_size=4, activations=['relu', None])\n",
    "t = Trainer(nn, verbose=False, loss_name='cross-entropy')\n",
    "for i in range(1000):\n",
    "    for batch_x, batch_y in batches(x, y, 16):\n",
    "        t.train(batch_x, batch_y)\n",
    "    if i % 10 == 0:\n",
    "        loss, metrics = t.eval(x_test, y_test, metrics=['accuracy'])\n",
    "        print('Test loss = %.5f, accuracy %.5f' % (loss, metrics[0]))\n",
    "\n",
    "t.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mnist.data / np.max(mnist.data)\n",
    "y = mnist.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers=[512, 256, 10], input_size=784, activations=['relu', 'relu', None])\n",
    "t = Trainer(nn, verbose=False, loss_name='cross-entropy', learning_rate=0.001)\n",
    "for i in range(20):\n",
    "    for batch_x, batch_y in batches(x, y, 64):\n",
    "        t.train(batch_x, batch_y)\n",
    "    if i % 1 == 0:\n",
    "        loss, metrics = t.eval(x_test, y_test, metrics=['accuracy'])\n",
    "        print('Test loss = %.5f, accuracy %.5f' % (loss, metrics[0]))\n",
    "\n",
    "t.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
